#!/usr/bin/env python
""" Functions to read a web page and to get a set of links in it.
recursive_links_helper.py rev. 15 Aug 2013 by Stuart Ambler,
Copyright (c) 2013 Stuart Ambler.
Tested with Python 2.7.3 and 3.2.3.
Requires installation of BeautifulSoup4.
To use functions, import recursive_links_helper.
After changing this file, reload(recursive_links_helper).
Distributed under the Boost License:
Boost Software License - Version 1.0 - August 17th, 2003

Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:

The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
"""
from __future__ import print_function
from bs4 import BeautifulSoup, SoupStrainer
import re
import sys
from time import sleep
if sys.version[0] < "3":
    import httplib
    from urllib2 import urlopen
    from urllib2 import HTTPError, URLError
    from urlparse import urljoin, urlparse, urlsplit
else:
    import http.client
    from urllib.request import urlopen
    from urllib.error import HTTPError, URLError
    from urllib.parse import urljoin, urlparse, urlsplit

# global constants

ONLY_A_TAGS = SoupStrainer("a")
HTTP_STR = r'^http://'
HTTP_REO = re.compile (HTTP_STR, re.I)

def read_url(url):
    """ Open and read web page.
    Args: url.
    Returns: bool success flag, web page.
    """
    # The only exceptions encountered so far as of 2013 Aug is HTTPError and
    # URLError.  That is, the code for the other exceptions is untested.
    page = ""
    read_ok = False
    try:
        response = urlopen(url)
        page     = response.read()
        read_ok  = True
        response.close()
    except http.client.BadStatusLine as exc:
        sys.stderr.write('BadStatusLine: ' + str(exc.reason) + "\n"
                         + url + "\n")
        sys.stderr.flush()
    except http.client.HTTPException as exc:
        sys.stderr.write('HTTPException: ' + str(exc.reason) + "\n"
                         + url + "\n")
        sys.stderr.flush()
    except HTTPError as exc:
        sys.stderr.write("HTTP Error: "    + str (exc.reason) + "\n"
                         + url + "\n")
        sys.stderr.flush()
    except URLError as exc:
        sys.stderr.write('URLError: '      + str(exc.reason) + "\n"
                         + url + "\n")
        sys.stderr.flush()
    return read_ok, page


def read_url_with_retry(url, nr_tries = 2, delay_sec = 1):
    """ Call read_url up to nr_tries times with delay_sec sleep in between.
    Args: url, nr_tries, delay_sec.
    Returns: bool success flag, web page.
    """
    split_object = urlsplit(url)
    # only attempt to read urls with certain schemes
    if split_object.scheme not in [ "file", "http", "https", "shttp"]:
        print("not following %s" % url)
        return False, ""
    for try_nr in range(1, nr_tries + 1):
        read_ok, page = read_url(url)
        if read_ok and page != "":
            break
        else:
            sleep(delay_sec)
    return read_ok, page


def get_links(page, page_url):
    """ Gets link urls (<a href values made into full urls) on a web page.
    Args: page, a string
          page_url, a hopefully full url for the page
    Returns: set of strings containing urls.  Using a set rather than a
             list eliminates duplicates.
    """
    soup = BeautifulSoup(page, "html.parser", parse_only=ONLY_A_TAGS)
    return set(map(lambda link: urljoin(page_url, link.get("href")),
                   soup.find_all("a")))

shorten = "http://csiflabs.cs.ucdavis.edu/~cs20/recursion_test/test1.html"



url_dict = dict()
url_set = shorten


def recursive_links(url_dict, url_set, max_level, level):
    if level >= max_level:
        print(url_dict)
        return
    pair = read_url(url_set)
    dump = get_links(pair[1], url_set)
    for link in dump: url_dict[link] = level
    level += 1
    for link in dump:
        recursive_links(url_dict, link, max_level, level)
    
       
    
    
    
recursive_links(url_dict, shorten, 6, 1)  
    



